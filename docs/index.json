[{"authors":null,"categories":null,"content":"个人简介, 控制在600-800个英文字符之内\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4e73f707a3c1da0c5d8d165361161c7b","permalink":"/authors/19_ruize/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/19_ruize/","section":"authors","summary":"个人简介, 控制在600-800个英文字符之内","tags":null,"title":"Ruize Xu","type":"authors"},{"authors":null,"categories":null,"content":"Guangyao is a Ph.D. Candidate at GeWu-Lab, Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. He got his master degree at China Agricultural University in 2020 and got into GeWu-Lab since then. His recently research interests include audio-visual learning and scene understanding. And he hopes to brave the no-man\u0026rsquo;s land on the road of scientific research and make warm artificial intelligence research! People who are interested in my research domain are very welcome and do not hesitate to contact me actively. For more information, please visit his personal homepage. Valar Morghulis！\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"537de72d4cb178cea6fbf2b2a92ea589","permalink":"/authors/20_guangyao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/20_guangyao/","section":"authors","summary":"Guangyao is a Ph.D. Candidate at GeWu-Lab, Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. He got his master degree at China Agricultural University in 2020 and got into GeWu-Lab since then. His recently research interests include audio-visual learning and scene understanding. And he hopes to brave the no-man\u0026rsquo;s land on the road of scientific research and make warm artificial intelligence research! People who","tags":null,"title":"Guangyao Li","type":"authors"},{"authors":null,"categories":null,"content":"Xiaokang is a master student in GeWu-Lab at Renmin University of China, advised by Prof. Di Hu. He got his undergraduate degree at School of Information, Renmin University of China in 2020 and got into GeWu-Lab since then. He is interested in multi-modal learning and perception, and optimization mechanism design. And he is also devoted to help these visually impaired with AI in both technology and practice.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"22debf3f166bda4bfb28c8317489f918","permalink":"/authors/20_xiaokang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/20_xiaokang/","section":"authors","summary":"Xiaokang is a master student in GeWu-Lab at Renmin University of China, advised by Prof. Di Hu. He got his undergraduate degree at School of Information, Renmin University of China in 2020 and got into GeWu-Lab since then. He is interested in multi-modal learning and perception, and optimization mechanism design. And he is also devoted to help these visually impaired with AI in both technology and practice.","tags":null,"title":"Xiaokang Peng","type":"authors"},{"authors":null,"categories":null,"content":"个人简介, 控制在600-800个英文字符之内\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"55a49bcd8ae300a0362a45302ca97c26","permalink":"/authors/20_xuemin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/20_xuemin/","section":"authors","summary":"个人简介, 控制在600-800个英文字符之内","tags":null,"title":"Xuemin Liu","type":"authors"},{"authors":null,"categories":null,"content":"Yixin is a master student at Gaoling School of Artificial Intelligence, Renmin University of China. His main research topics are Multi-modal Scene Perception and Self-surpervised Representation Learning. Now he is working on video understanding and speaker diarization task for complex speech scenario. He is also interested in Internet finance, and has got his Bachelor of Finance in Renmin University of China besides the Computer Science degree.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"033ae9c233d8ca15172e0f0eb482735e","permalink":"/authors/20_yixin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/20_yixin/","section":"authors","summary":"Yixin is a master student at Gaoling School of Artificial Intelligence, Renmin University of China. His main research topics are Multi-modal Scene Perception and Self-surpervised Representation Learning. Now he is working on video understanding and speaker diarization task for complex speech scenario. He is also interested in Internet finance, and has got his Bachelor of Finance in Renmin University of China besides the Computer Science degree.","tags":null,"title":"Yixin Xu","type":"authors"},{"authors":null,"categories":null,"content":"Rui is interested in computer vision and machine learning, and has done some research on video representation learning and joint audio-visual learning. During his undergraduate he works with Prof. Di Hu. Now Rui is a Ph.D. student in Multi-Media Lab at The Chinese University of Hong Kong, supervised by Prof. Dahua Lin.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"9434b9dca31f1f23a676f2b869e0c881","permalink":"/authors/21_ruiqian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/21_ruiqian/","section":"authors","summary":"Rui is interested in computer vision and machine learning, and has done some research on video representation learning and joint audio-visual learning. During his undergraduate he works with Prof. Di Hu. Now Rui is a Ph.D. student in Multi-Media Lab at The Chinese University of Hong Kong, supervised by Prof. Dahua Lin.","tags":null,"title":"Rui Qian","type":"authors"},{"authors":null,"categories":null,"content":"Yake is a PhD student at Gaoling School of Artificial Intelligence, Renmin University of China. She received her bachelor\u0026rsquo;s degree in Computer Science and Technology from University of Electronic Science and Technology of China in 2021. Now her research interests focus on the effective mechanism of multi-modal learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"57b1d4e29185f3870d53fc65c766173e","permalink":"/authors/21_yake/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/21_yake/","section":"authors","summary":"Yake is a PhD student at Gaoling School of Artificial Intelligence, Renmin University of China. She received her bachelor\u0026rsquo;s degree in Computer Science and Technology from University of Electronic Science and Technology of China in 2021. Now her research interests focus on the effective mechanism of multi-modal learning.","tags":null,"title":"Yake Wei","type":"authors"},{"authors":null,"categories":null,"content":"Andong Deng spent a wonderful year at GeWu Lab doing research about multimodal learning with Dr. Di Hu from 2021 to 2022. Now he is an upcoming PhD student in 2022 Fall at Center for Research in Computer Vision, University of Central Florida, advised by Dr. Chen Chen. His research interests include multi-modal learning, video understanding and 3D vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"c95476ad24cc214056b3d2c5e8c90f17","permalink":"/authors/22_andong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/22_andong/","section":"authors","summary":"Andong Deng spent a wonderful year at GeWu Lab doing research about multimodal learning with Dr. Di Hu from 2021 to 2022. Now he is an upcoming PhD student in 2022 Fall at Center for Research in Computer Vision, University of Central Florida, advised by Dr. Chen Chen. His research interests include multi-modal learning, video understanding and 3D vision.","tags":null,"title":"Andong Deng","type":"authors"},{"authors":null,"categories":null,"content":"Wenke is a Ph.D student since 2022 Fall at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. His research interests include reinforcement learning and embodied AI. Now, he focus on building a generalizable manipulation policy with computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a2791369e75b13b52139d9860293bdd5","permalink":"/authors/22_wenke/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/22_wenke/","section":"authors","summary":"Wenke is a Ph.D student since 2022 Fall at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. His research interests include reinforcement learning and embodied AI. Now, he focus on building a generalizable manipulation policy with computer vision.","tags":null,"title":"Wenke Xia","type":"authors"},{"authors":null,"categories":null,"content":"Wenxuan is a Ph.D student in the GeWu-Lab, Gaoling School of Artificial Intelligence, Renmin University of China. He has got his bachelor\u0026rsquo;s degree and master\u0026rsquo;s degree in Northwestern Polytechnical University and Xi\u0026rsquo;an Jiaotong University, respectively. Now his main research focuses on multimodal learning towards real-world scene understanding, aiming to guide the machine to perceive and understand natural scenes like human beings.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"cd37724dba9b446f1c1307e40cd45632","permalink":"/authors/22_wenxuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/22_wenxuan/","section":"authors","summary":"Wenxuan is a Ph.D student in the GeWu-Lab, Gaoling School of Artificial Intelligence, Renmin University of China. He has got his bachelor\u0026rsquo;s degree and master\u0026rsquo;s degree in Northwestern Polytechnical University and Xi\u0026rsquo;an Jiaotong University, respectively. Now his main research focuses on multimodal learning towards real-world scene understanding, aiming to guide the machine to perceive and understand natural scenes like human beings.","tags":null,"title":"Wenxuan Hou","type":"authors"},{"authors":null,"categories":null,"content":"Xincheng is a master student in GeWu-Lab at Renmin University of China, advised by Prof. Di Hu. Currently his research interests focus on scene understanding in embodied ai with multi-modal.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"a389590984a0c3fb50de499f8df2d4c0","permalink":"/authors/22_xincheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/22_xincheng/","section":"authors","summary":"Xincheng is a master student in GeWu-Lab at Renmin University of China, advised by Prof. Di Hu. Currently his research interests focus on scene understanding in embodied ai with multi-modal.","tags":null,"title":"Xincheng Pang","type":"authors"},{"authors":null,"categories":null,"content":"Zequn is a Ph.D. student at GeWu-Lab, Gaoling School of Artificial Intelligence, Renmin University of China. He currently focuses on the mechanism of multi-modal learning, including theoretical comprehension and algorithm design. He also has a keen interest in developing efficient and effective multi-view clustering techniques utilizing machine learning methods.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d884fc3eb1e2b2382def5073cec5e105","permalink":"/authors/22_zequn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/22_zequn/","section":"authors","summary":"Zequn is a Ph.D. student at GeWu-Lab, Gaoling School of Artificial Intelligence, Renmin University of China. He currently focuses on the mechanism of multi-modal learning, including theoretical comprehension and algorithm design. He also has a keen interest in developing efficient and effective multi-view clustering techniques utilizing machine learning methods.","tags":null,"title":"Zequn Yang","type":"authors"},{"authors":null,"categories":null,"content":"Henghui is a master student in GeWu-Lab at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. He has got his bachelor\u0026rsquo;s degree in Dalian University of Technology in 2023. Currently his research instrests focus on Multi-modal Large Language Models for audio-visual scene understanding.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"0f875044223f8afd458b089859ba38d8","permalink":"/authors/23_henghui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_henghui/","section":"authors","summary":"Henghui is a master student in GeWu-Lab at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. He has got his bachelor\u0026rsquo;s degree in Dalian University of Technology in 2023. Currently his research instrests focus on Multi-modal Large Language Models for audio-visual scene understanding.","tags":null,"title":"Henghui Du","type":"authors"},{"authors":null,"categories":null,"content":"Jiahao is a senior student of the School of Computer Science and Engineering, BUAA. He is interested in the interaction mechanism of multi-modal.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"8808a5aa1460c5cb4fad660d28f8520a","permalink":"/authors/23_jiahao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_jiahao/","section":"authors","summary":"Jiahao is a senior student of the School of Computer Science and Engineering, BUAA. He is interested in the interaction mechanism of multi-modal.","tags":null,"title":"Jiahao Li","type":"authors"},{"authors":null,"categories":null,"content":"Jingxian is a fourth-year student of Gaoling School of Artificial Intelligence, Renmin University of China. He is interested in robot manipulation and perception from interaction.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"bdeafc1f9127d19078299ad17ddcf547","permalink":"/authors/23_jingxian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_jingxian/","section":"authors","summary":"Jingxian is a fourth-year student of Gaoling School of Artificial Intelligence, Renmin University of China. He is interested in robot manipulation and perception from interaction.","tags":null,"title":"Jingxian Lu","type":"authors"},{"authors":null,"categories":null,"content":"Juncheng is a third-year student of School of Artificial Intelligence, University of Chinese Academy of Sciences. His research interests include audio-visual localization and segmentation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"874c09024781e4fd5375423eaef9c9e8","permalink":"/authors/23_juncheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_juncheng/","section":"authors","summary":"Juncheng is a third-year student of School of Artificial Intelligence, University of Chinese Academy of Sciences. His research interests include audio-visual localization and segmentation.","tags":null,"title":"Juncheng Ma","type":"authors"},{"authors":null,"categories":null,"content":"个人简介, 控制在600-800个英文字符之内\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"834fe556c30cd4180a6dc4c692fd63d9","permalink":"/authors/23_liangce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_liangce/","section":"authors","summary":"个人简介, 控制在600-800个英文字符之内","tags":null,"title":"Ce Liang","type":"authors"},{"authors":null,"categories":null,"content":"Peiwen is a second-year MPhil student of the Department of Artificial Intelligence, Beijing University of Posts and Telecommunications. He is interested in multimodal learning including sentiment, segmentation and foundation models.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"bf84fe39ef0b614af0ae82d08359c784","permalink":"/authors/23_peiwen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_peiwen/","section":"authors","summary":"Peiwen is a second-year MPhil student of the Department of Artificial Intelligence, Beijing University of Posts and Telecommunications. He is interested in multimodal learning including sentiment, segmentation and foundation models.","tags":null,"title":"Peiwen Sun","type":"authors"},{"authors":null,"categories":null,"content":"Ruoxuan is a third-year master student in GeWu-Lab at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. He has got his bachelor’s degree in Hunan University in 2023. He is interested in multi-modal embodied AI, focusing on enabling robots to perceive and interact with the physical world through vision, audition, touch, and force sensing.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"c29a63de0242659b43a43451fc077046","permalink":"/authors/23_ruoxuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_ruoxuan/","section":"authors","summary":"Ruoxuan is a third-year master student in GeWu-Lab at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. He has got his bachelor’s degree in Hunan University in 2023. He is interested in multi-modal embodied AI, focusing on enabling robots to perceive and interact with the physical world through vision, audition, touch, and force sensing.","tags":null,"title":"Ruoxuan Feng","type":"authors"},{"authors":null,"categories":null,"content":"个人简介, 控制在600-800个英文字符之内\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"364786f50ed04bbfb2309f8069cdbe90","permalink":"/authors/23_shaoxuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_shaoxuan/","section":"authors","summary":"个人简介, 控制在600-800个英文字符之内","tags":null,"title":"Shaoxuan Xu","type":"authors"},{"authors":null,"categories":null,"content":"Siwei is a fourth-year student of the Department of Electronic Engineering, Tsinghua University. He is interested in image editing with generative diffusion models and image deblurring.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"062e874f9d4216ee7c15e6afe41e1631","permalink":"/authors/23_siwei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_siwei/","section":"authors","summary":"Siwei is a fourth-year student of the Department of Electronic Engineering, Tsinghua University. He is interested in image editing with generative diffusion models and image deblurring.","tags":null,"title":"Siwei Li","type":"authors"},{"authors":null,"categories":null,"content":"Yaoting is currently working as an intern at the Deepwise AI Lab for multimodal medical data processing. He received his master\u0026rsquo;s degree from the University of Edinburgh in 2022. His research interests include multimodal deep learning, cross-modal transformers, and affective computing.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"bda305ecfaa132f6e49d2dd2566d0f25","permalink":"/authors/23_yaoting/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/23_yaoting/","section":"authors","summary":"Yaoting is currently working as an intern at the Deepwise AI Lab for multimodal medical data processing. He received his master\u0026rsquo;s degree from the University of Edinburgh in 2022. His research interests include multimodal deep learning, cross-modal transformers, and affective computing.","tags":null,"title":"Yaoting Wang","type":"authors"},{"authors":null,"categories":null,"content":"Chengxiang is a fourth-year undergraduate student of Beijing University of Posts and Telecommunications. He is interested in multimodal learning and currently focuses on the multimodal imbalance problem.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"33776e2ec813d12a4629e831ad94bf1f","permalink":"/authors/24_chengxiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/24_chengxiang/","section":"authors","summary":"Chengxiang is a fourth-year undergraduate student of Beijing University of Posts and Telecommunications. He is interested in multimodal learning and currently focuses on the multimodal imbalance problem.","tags":null,"title":"Chengxiang Huang","type":"authors"},{"authors":null,"categories":null,"content":"HaoTian Ni is a third-year undergraduate student at the School of Computer Science and Engineering (SCSE), Beihang University (BUAA). He joined GeWu-Lab as a research assistant in July 2024, with research interests in multimodal learning and generative models.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"ab22c2e48161201b773b0e90c4d95e54","permalink":"/authors/24_haotian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/24_haotian/","section":"authors","summary":"HaoTian Ni is a third-year undergraduate student at the School of Computer Science and Engineering (SCSE), Beihang University (BUAA). He joined GeWu-Lab as a research assistant in July 2024, with research interests in multimodal learning and generative models.","tags":null,"title":"Haotian Ni","type":"authors"},{"authors":null,"categories":null,"content":"Jirui is a second-year MPhil student of the School of Computer and Artificial Intelligence, Wuhan University of Technology. She is interested in multimodal understanding and cross-modal generation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7a6ee1988cb2fa93bfeee88a094c7489","permalink":"/authors/24_jirui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/24_jirui/","section":"authors","summary":"Jirui is a second-year MPhil student of the School of Computer and Artificial Intelligence, Wuhan University of Technology. She is interested in multimodal understanding and cross-modal generation.","tags":null,"title":"JiRui Huang","type":"authors"},{"authors":null,"categories":null,"content":"个人简介, 控制在600-800个英文字符之内\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f7261fbcc38cc2bc051fd1cdf55be92d","permalink":"/authors/24_menglu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/24_menglu/","section":"authors","summary":"个人简介, 控制在600-800个英文字符之内","tags":null,"title":"Menglu Cui","type":"authors"},{"authors":null,"categories":null,"content":"个人简介, 控制在600-800个英文字符之内\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"def658f6e3cf6e584dd3b8f3f4fbe4fb","permalink":"/authors/24_xingshuo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/24_xingshuo/","section":"authors","summary":"个人简介, 控制在600-800个英文字符之内","tags":null,"title":"Xingshuo Zhang","type":"authors"},{"authors":null,"categories":null,"content":"Yuchen is a master student in GeWu-Lab at Renmin University of China, advised by Prof. Di Hu. He has got his bachelor\u0026rsquo;s degree in Zhejiang University in 2024.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"186e15560cfa29bcd45c618efc625779","permalink":"/authors/24_yuchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/24_yuchen/","section":"authors","summary":"Yuchen is a master student in GeWu-Lab at Renmin University of China, advised by Prof. Di Hu. He has got his bachelor\u0026rsquo;s degree in Zhejiang University in 2024.","tags":null,"title":"Yuchen Li","type":"authors"},{"authors":null,"categories":null,"content":"Guikai Lin is a third-year undergraduate student at School of Information, Renmin University of China. She is interested in robot manipulation and embodied AI.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7d388187c6128ca0cdb7969810ea5c0c","permalink":"/authors/25_guikai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_guikai/","section":"authors","summary":"Guikai Lin is a third-year undergraduate student at School of Information, Renmin University of China. She is interested in robot manipulation and embodied AI.","tags":null,"title":"Guikai Lin","type":"authors"},{"authors":null,"categories":null,"content":"Kailin is a third-year undergraduate student at the School of Computer Science, Wuhan University. He is currently interested in multimodal learning, particularly the multimodal imbalance problem.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3e8a31123abb917c8042a276bf32bbf5","permalink":"/authors/25_kailin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_kailin/","section":"authors","summary":"Kailin is a third-year undergraduate student at the School of Computer Science, Wuhan University. He is currently interested in multimodal learning, particularly the multimodal imbalance problem.","tags":null,"title":"Kailin Huang","type":"authors"},{"authors":null,"categories":null,"content":"Mingxin is a first-year Ph.D. student at the GeWu-Lab, Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. She obtained her master’s degree from University of Electronic Science and Technology of China in 2025. She is also jointly trained by the Beijing Academy of Artificial Intelligence (BAAI). Her research interests focus on multimodal interaction and embodied intelligence.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"864abb65c2d65e996e381b37b8f742af","permalink":"/authors/25_mingxin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_mingxin/","section":"authors","summary":"Mingxin is a first-year Ph.D. student at the GeWu-Lab, Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. She obtained her master’s degree from University of Electronic Science and Technology of China in 2025. She is also jointly trained by the Beijing Academy of Artificial Intelligence (BAAI). Her research interests focus on multimodal interaction and embodied intelligence.","tags":null,"title":"Mingxin Wang","type":"authors"},{"authors":null,"categories":null,"content":"Dongnuan is a Ph.D student since 2025 Fall at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. He received his bachelor’s degree in South China University of Technology. He is interested in multimodal understanding and multimodal large language models.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"cb61e37772883556e00892e2efd5804d","permalink":"/authors/25_ongnuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_ongnuan/","section":"authors","summary":"Dongnuan is a Ph.D student since 2025 Fall at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. He received his bachelor’s degree in South China University of Technology. He is interested in multimodal understanding and multimodal large language models.","tags":null,"title":"Dongnuan Cai","type":"authors"},{"authors":null,"categories":null,"content":"Siqi is a fourth-year student of China University of Geosciences, Wuhan and incoming master of Institute of Software China Academy of Sciences. Now she is interested in embodied AI.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"b3173fbf48415a654e6edbc4ac52b4f0","permalink":"/authors/25_siqi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_siqi/","section":"authors","summary":"Siqi is a fourth-year student of China University of Geosciences, Wuhan and incoming master of Institute of Software China Academy of Sciences. Now she is interested in embodied AI.","tags":null,"title":"Siqi Li","type":"authors"},{"authors":null,"categories":null,"content":"Siyu Mei is a fourth-year undergraduate student at Beijing Jiaotong University. He is interested in multimodal interaction and perception.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"87c353ba35be1d7fd094deb0bb232a89","permalink":"/authors/25_siyu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_siyu/","section":"authors","summary":"Siyu Mei is a fourth-year undergraduate student at Beijing Jiaotong University. He is interested in multimodal interaction and perception.","tags":null,"title":"Siyu Mei","type":"authors"},{"authors":null,"categories":null,"content":"Hao Wang is a third-year undergraduate student at China University of Geosciences (Beijing). He joined GeWu-Lab in July 2025, and his research interests include robot manipulation and embodied AI.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f8c083df56997820626a04d772eb31e8","permalink":"/authors/25_wanghao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_wanghao/","section":"authors","summary":"Hao Wang is a third-year undergraduate student at China University of Geosciences (Beijing). He joined GeWu-Lab in July 2025, and his research interests include robot manipulation and embodied AI.","tags":null,"title":"Hao Wang","type":"authors"},{"authors":null,"categories":null,"content":"Weitao Zhang is a second-year undergraduate student at Gaoling School of Artificial Intelligence, Renmin University of China. He is interested in embodied intelligence and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"3dcba9b23c3d5864ae7100c9b04f7b6e","permalink":"/authors/25_weitao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_weitao/","section":"authors","summary":"Weitao Zhang is a second-year undergraduate student at Gaoling School of Artificial Intelligence, Renmin University of China. He is interested in embodied intelligence and computer vision.","tags":null,"title":"Weitao Zhang","type":"authors"},{"authors":null,"categories":null,"content":"Wenbo Yu is an undergraduate student at Beijing Forestry University. His research interests lie in foundational challenges within embodied intelligence, including Sim2Real transfer.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"f0e65812de78c44e10af6574be697685","permalink":"/authors/25_wenbo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_wenbo/","section":"authors","summary":"Wenbo Yu is an undergraduate student at Beijing Forestry University. His research interests lie in foundational challenges within embodied intelligence, including Sim2Real transfer.","tags":null,"title":"Wenbo Yu","type":"authors"},{"authors":null,"categories":null,"content":"Xiangyun is a third-year undergraduate student at School of Automation, Beijing Institute of Technology. He is interested in robotic multimodal arms.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"95986f218abf20a90a3dd5379c8d9e17","permalink":"/authors/25_xiangyun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_xiangyun/","section":"authors","summary":"Xiangyun is a third-year undergraduate student at School of Automation, Beijing Institute of Technology. He is interested in robotic multimodal arms.","tags":null,"title":"Xiangyun Lu","type":"authors"},{"authors":null,"categories":null,"content":"Youquan is a PhD student at Gaoling School of Artificial Intelligence, Renmin University of China. He received his bachelor\u0026rsquo;s degree in School of Artificial Intelligence, Xidian University in 2025. Now his research interests focus on the audio and visual.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"76878201f3e2f26fa9ded3454ab799f8","permalink":"/authors/25_youquan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_youquan/","section":"authors","summary":"Youquan is a PhD student at Gaoling School of Artificial Intelligence, Renmin University of China. He received his bachelor\u0026rsquo;s degree in School of Artificial Intelligence, Xidian University in 2025. Now his research interests focus on the audio and visual.","tags":null,"title":"Youquan Fu","type":"authors"},{"authors":null,"categories":null,"content":"Yu Miao is a first-year Ph.D. student at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. She received her bachelor\u0026rsquo;s degree in Beijing University of Posts and Telecommunications in 2025. Now she has a keen interest in multimodal large language models.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"8681f5e1dbad35eebd6db3c19c73258f","permalink":"/authors/25_yumiao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_yumiao/","section":"authors","summary":"Yu Miao is a first-year Ph.D. student at Gaoling School of Artificial Intelligence, Renmin University of China, advised by Prof. Di Hu. She received her bachelor\u0026rsquo;s degree in Beijing University of Posts and Telecommunications in 2025. Now she has a keen interest in multimodal large language models.","tags":null,"title":"Yu Miao","type":"authors"},{"authors":null,"categories":null,"content":"Yuxuan is a fourth-year undergraduate student at School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, and an incoming master student of Tsinghua Shenzhen International Graduate School. He joined GeWu-Lab as a research assistant in January 2025, and his research interests lie in tactile representation learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"885b6c3a1269038b87208bf85bafc39e","permalink":"/authors/25_yuxuan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/25_yuxuan/","section":"authors","summary":"Yuxuan is a fourth-year undergraduate student at School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, and an incoming master student of Tsinghua Shenzhen International Graduate School. He joined GeWu-Lab as a research assistant in January 2025, and his research interests lie in tactile representation learning.","tags":null,"title":"Yuxuan Zhou","type":"authors"},{"authors":null,"categories":null,"content":"Jifan Li is a fourth-year student at the School of Automation Science and Electrical Engineering(SASEE), Beihang University(BUAA). He will be a doctoral candidate in joint training with Beijing Academy of Artificial Intelligence(BAAI) in 2026 fall . His research focuses on embodied AI.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"7e76fb04d2f4d65c12a39000a870e820","permalink":"/authors/26_jifan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/26_jifan/","section":"authors","summary":"Jifan Li is a fourth-year student at the School of Automation Science and Electrical Engineering(SASEE), Beihang University(BUAA). He will be a doctoral candidate in joint training with Beijing Academy of Artificial Intelligence(BAAI) in 2026 fall . His research focuses on embodied AI.","tags":null,"title":"Jifan Li","type":"authors"},{"authors":null,"categories":null,"content":"Yizhuo is a fourth-year student of School of Computer Science and Technology, Jilin University. He is interested in embodied intelligence and world models.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"dbb5de3fca721bb951dd671bcad41aa2","permalink":"/authors/26_yizhuo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/26_yizhuo/","section":"authors","summary":"Yizhuo is a fourth-year student of School of Computer Science and Technology, Jilin University. He is interested in embodied intelligence and world models.","tags":null,"title":"Yizhuo Zhang","type":"authors"},{"authors":null,"categories":null,"content":"Ziheng is a fourth-year student of School of Computer Science and Technology, Jilin University. He is interested in multimodal large language models.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"dcd76c5b6dbbb23f899578e978926430","permalink":"/authors/26_ziheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/26_ziheng/","section":"authors","summary":"Ziheng is a fourth-year student of School of Computer Science and Technology, Jilin University. He is interested in multimodal large language models.","tags":null,"title":"Ziheng Chen","type":"authors"},{"authors":["dihu"],"categories":null,"content":"Di Hu is tenure-track faculty at Gaoling School of Artificial Intelligence, Renmin University of China. Before that, he was previously a research scientist at Baidu Research. Di Hu obtained the Ph.D degree from Northwestern Polytechnical University in 2019, supervised by Xuelong Li. Currently, Di Hu is leading the GeWu Lab and exploring how to understand and interact with the world via the natural multimodal messages. He is an aficionado of cognitive neuroscience and has wrote one study note during his undergraduate. Inspired by what he learned from cognitive neuroscience, and what he observed and deliberated from the daily-life, he strongly convinced that the pervasive, free, natural multimodal messages can provide sufficient information for perceiving, learning and understanding environment, even the agent itself, which promisingly makes multimodal learning become one of the key to achieve machine intelligence.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Di Hu is tenure-track faculty at Gaoling School of Artificial Intelligence, Renmin University of China. Before that, he was previously a research scientist at Baidu Research. Di Hu obtained the Ph.D degree from Northwestern Polytechnical University in 2019, supervised by Xuelong Li. Currently, Di Hu is leading the GeWu Lab and exploring how to understand and interact with the world via the natural multimodal messages. He is an aficionado of","tags":null,"title":"Di Hu","type":"authors"},{"authors":["Chengxiang Huang","Yake Wei"],"categories":null,"content":" What is modality imbalance? In human activities, we often use both hands together to perform a series of actions. For example, a skilled juggler can seamlessly toss objects between their hands in a rhythmic pattern. This action requires precise coordination between both hands. However, despite working together, the hands are not always equal in their roles—most people have a dominant hand. Right-handed individuals primarily rely on their right hand for control and dexterity, while left-handed individuals depend more on their left hand. This dominance is a natural physiological phenomenon.\nIf we want to further enhance the coordination between both hands—such as juggling multiple balls effortlessly like a professional performer—it requires extensive practice. Improving the proficiency of the non-dominant hand and refining synchronization between both hands are essential to achieving fluid and precise movements.\nNow, let’s draw a parallel with multimodal learning. In a multimodal learning system, multiple modalities serve as inputs, working together within a model to accomplish a shared objective and generate an output. But do certain modalities take on a dominant role, just like our hands? And does this dominant modality similarly suppress the collaborative efficiency of the others? Through empirical observations and theoretical analysis, the multimodal community has found that the answer is YES [1][2].\nFig 1. Classic multimodal learning paradigm.\nAs shown in Fig 1, in a classic multimodal learning paradigm, consider the typical case of two modalities. Data from each modality is first processed through its respective encoder to obtain unimodal representations. These representations are then interacted and integrated before being fed into a predictor. The entire multimodal model is optimized based on the learning objective.\nFig 2. An audio-video event labeled as bus.\nTherefore, in the classical multimodal learning paradigm, we typically train different modalities jointly. However, we must recognize that data from different modalities exhibit inherent heterogeneity. For instance, consider an audio-video pair labeled as bus in Fig 2. In the visual modality, the bus is clearly visible, making it relatively easy to learn the concept of bus. In contrast, the audio modality may capture a variety of environmental sounds from the street, which can be noisy and less informative for distinguishing the bus category. We find that this disparity in learning characteristics across modalities introduces potential challenges for effective multimodal collaboration.\nFig 3. Unimodal components determine the feed-forward and back-propagation stages in the \"additive\" manner. Take modality a and v as an example.\nThe paper OGM (Balanced Multimodal Learning via On-the-fly Gradient Modulation) [3] conducted an analysis for the optimization process of classic multimodal joint learning. Here we take two modalities, $a$ and $v$ as an example. Note that this analysis can be extended to $M$ modalities case, which is provided in T-PAMI 2024 paper [4].\nSuppose each $x_{i}$ contains inputs of two modalities: $x_{i}=(x^{a}_{i}, x^{v}_{i})$. $y_{i} \\in \\{1,2,\\cdots,C\\}$ is the target label of sample $x_i$ and $C$ is the number of categories. For modality $m$, where $m \\in \\{a,v\\}$, its input is processed by the corresponding encoder $\\varphi^{m}(\\theta^{m},\\cdot)$. $\\theta^{m}$ are the parameters of the encoder. After extraction, their features are fused via concatenation (which is one of the most widely used fusion strategies), and passed to a single-layer linear classifier. $W \\in \\mathbb{R}^{C \\times \\sum^M_{m=1} d_{\\varphi^m}}$ and $b \\in \\mathbb{R}^{C}$ denote the parameters of the linear classifier. $d_{\\varphi^m}$ is the output dimension of $\\varphi^{m}(\\theta^{m},\\cdot)$. Then, we can formulate the multimodal prediction $f(x_i)$ of the feed-forward stage, and the gradient $\\frac{\\partial L }{\\partial f(x_{i})_c}$ as shown in Fig 3.\nWe can have the following observations:\n 🛠 Key Observations:\n• In the feed-forward stage, the final output is determined by the \u0026ldquo;additive\u0026rdquo; combination of unimodal components.\n• In the back-propagation stage, the gradient is also influenced by the \u0026ldquo;additive\u0026rdquo; combination of unimodal components.\n Now, let\u0026rsquo;s consider the heterogeneity of multimodal data.\nSuppose one modality—such as the cleaner and easier-to-learn visual modality in the former case—consistently provides higher-quality representations and assigns high confidence to the correct category. Regardless of the other modality\u0026rsquo;s prediction, the final combined result remains correct since the additive combination.\nAs a result, the easier-to-learn modality tends to dominate and control the overall learning dynamics of the multimodal model. This dominance suppresses the optimization of other modalities, leading to underutilization. Ultimately, this imbalance limits the full potential of the multimodal model, highlighting the critical challenge of achieving balanced multimodal learning.\nHow to alleviate modality imbalance? Fig 4. The taxonomy of methods for multimodal imbalance learning. This figure is from [5].\nAfter identifying the problem, the next step is to explore potential solutions. For this fundamental challenge of modality imbalance, the goal is not merely to design a specific model architecture. Instead, attention has gradually shifted toward different stages of the training process, with the aim of developing algorithms that are transferable across a wide range of multimodal learning scenarios.\nTo provide a clearer understanding of existing solutions, we categorize representative methods into four key stages of multimodal training: Data input, Feed-forward, Learning objective, and Optimization, as illustrated in Fig 4. The following sections provide a detailed discussion of approaches within each category.\nData input The quality of multimodal data is crucial for effective multimodal learning. However, for any given multimodal sample (e.g., a data point composed of visual and audio modalities), there often exists a disparity in the quality or informativeness between the constituent modalities. This disparity can cause the model to develop a preference for the modality that is easier to learn, which subsequently tends to dominate the overall learning dynamics and suppress the optimization of other modalities.\nFig 5. An illustration of sample-level discrepancies between modalities. This figure is from [6].\nImagine a video and audio clip describing a motorcycle; the amount of information contained in each modality can differ significantly. For example, as shown in Fig. 5(a), the image itself contains very limited information, while in Fig. 5(b), the visual clearly shows a motorcycle wheel. This per-sample information disparity leads to varying contributions to model prediction. As illustrated in Fig. 5(c), when the visual modality carries little information (Fig. 5(a)), the model tends to rely more on the audio modality, whereas when the visual modality is informative (Fig. 5(b)), the model is more inclined to rely on visual cues.\nTo address the imbalance originating from this sample-level quality disparity, the work Enhancing Multimodal Cooperation via Sample-level Modality Valuation [6] proposes a targeted solution. The key insight is to shift the focus from a \u0026ldquo;macro\u0026rdquo; or dataset-level view of data quality to a fine-grained sample-level perspective. The approach first introduces a valuation metric grounded in the Shapley value from game theory to precisely measure the contribution of each modality for every individual sample. Once the modality with a lower contribution is identified for a specific sample, a targeted re-sampling strategy is employed. This provides focused, additional training for the deficient modality, compensating for its initial lack of informativeness and thereby promoting a more balanced and effective multimodal collaboration.\nFeed-forward Adjustments during the feed-forward stage determine how a multimodal model perceives and integrates information from different modalities during prediction.\nA number of studies intervene at this stage to enhance the processing of information from weaker modalities, thereby mitigating the dominance of stronger ones in multimodal learning [4].\nFor example, Multimodal Learning with Alternating Unimodal Adaptation (MLA) [7] directly addresses modality imbalance by altering the feed-forward stage during training. Instead of a conventional feed-forward pass where inputs from all modalities are processed jointly, MLA\u0026rsquo;s alternating unimodal learning framework constrains each feed-forward operation to a single modality at a time. In any given training step, the network sees and computes an output based on only one data stream, which prevents signals from dominant modalities from interfering with or suppressing weaker ones. This isolated feed-forward process ensures a more balanced representation is learned before backpropagation, while a supporting gradient modification technique ensures the shared classifier doesn\u0026rsquo;t forget previously seen modalities. Apart from MLA, several other methods also intervene at the feed-forward stage to tackle modality imbalance. For instance, Adaptive Mask Co-optimization (AMCo) [8] dynamically masks features of the dominant modality, while On-the-fly Prediction Modulation (OPM) [4] drops some of the dominant modality\u0026rsquo;s features. Meanwhile, Greedy [9] enhances multimodal fusion by leveraging the MMTM architecture to facilitate better modality interaction.\nLearning objective Whether from the perspective of data input or the feed-forward stage, these approaches focus on adjusting multimodal information before or during its entry into the model. However, they do not explicitly encourage the model to move toward modality balance during the update. In contrast, another group of methods [10] [11] [12] [13] [14] [15] [16] focuses on the learning objective itself to alleviate modality imbalance.\nOne representative example of learning objective–level intervention is MMPareto (Boosting Multimodal Learning with Innocent Unimodal Assistance) [10]. Unlike previous methods that adjust the input data or modify how the model processes different modalities during the feed-forward stage, MMPareto focuses directly on how the model updates its parameters , that is, on the learning objective itself.\nFig 6. Illustration of multimodal framework and gradient integration strategy of MMPareto [8].\nA standard multimodal model is often trained with a single, shared objective function that evaluates performance after all modalities have been fused. However, to better supervise each modality\u0026rsquo;s representation and prevent weaker ones from being neglected, some methods enhance this setup. They adopt a multitask-like framework, as depicted in Figure 6, which supplements the main multimodal objective with separate objectives for each individual modality (unimodal). While this helps weaker modalities learn better, it also creates a potential problem: the optimization goals of these different losses can conflict with each other. For example, the model might receive contradictory signals from the multimodal and unimodal objectives, leading to unstable or suboptimal training.\nMMPareto addresses this by carefully analyzing and combining these gradients (the directions in which the model updates its parameters). It draws inspiration from Pareto optimization, a technique that aims to find a balance when optimizing multiple goals. Specifically, MMPareto ensures that the final update direction benefits both the multimodal and unimodal objectives without letting one overpower the other. At the same time, it adjusts the strength of these updates to help the model learn more generalizable patterns and avoid overfitting.\nIn simple terms, instead of just telling the model “learn from both objectives,” MMPareto tells it how to learn from both in a coordinated and balanced way.\nIn addition to MMPareto, several other methods also tackle modality imbalance by adjusting the learning objective. For example, Multimodal Cosine Loss (MMCosine) [11] introduces a multimodal cosine loss to reduce the gap in weight norms between different modalities and applies inter-symmetric constraints to encourage balanced and collaborative learning. MBSD [12] adopts a knowledge distillation framework, transferring the knowledge (predicted probability distributions and high-level representations) learned by the dominant modality to the weaker one. Calibrating Multimodal Learning (CML) [13] introduces a confidence loss for each modality, reducing the confidence of the dominant modality to alleviate imbalance. LFM [14] incorporates contrastive learning losses for each modality to mitigate differences in their ability to fit category labels, thereby addressing the imbalance at the supervision level.\nAnother related line of work enhances multimodal learning by introducing unimodal-specific training objectives. Uni-Modal Teacher (UMT) [15] distills unimodal feature-level outputs from standalone unimodal models into the corresponding unimodal branches within the multimodal model, helping each modality learn more effectively. Gradient-Blending (GBlending) [16] dynamically calculates optimal fusion weights based on the overfitting behavior of each modality, guiding the model\u0026rsquo;s training accordingly.\nOptimization Beyond modifying the data input, feed-forward process, or learning objectives, another important direction focuses on optimization itself. Specifically, adjusting how gradients from different modalities are handled during training. These gradient-based strategies aim to ensure that each modality receives an appropriate learning signal, rather than being overpowered by stronger modalities during backpropagation.\nA representative method in this category is On-the-fly Gradient Modulation (OGM) [3]. This method is based on a key observation. As shown in Fig 3, the gradients associated with weaker modalities tend to be suppressed during training, largely because the dominant modality\u0026rsquo;s high-confidence predictions (i.e., higher logits) overshadow them. As a result, weaker modalities receive fewer meaningful updates and struggle to improve. OGM addresses this by slowing down the gradient updates of dominant modalities, effectively allowing the weaker ones to \u0026ldquo;catch up.\u0026rdquo;\nFig 7. The pipeline of the On-the-fly Gradient Modulation strategy [3].\nTo do this, OGM dynamically monitors the discrepancy between modalities’ contributions to the prediction at each training step. If one modality (e.g., audio) consistently produces more confident predictions than the other (e.g., visual), OGM interprets it as being over-optimized. It then applies a modulation factor to scale down that modality’s gradient before the model updates its parameters.\nThis ensures that the underperforming modality receives relatively stronger gradient signals, giving it more opportunity to improve. Importantly, this is done without changing the model architecture or requiring additional modules, and the entire process happens \u0026ldquo;on the fly\u0026rdquo; during training.\nHowever, reducing gradients can also weaken the beneficial randomness (i.e., stochastic noise) that helps models generalize well. To counteract this, OGM introduces Gaussian noise into the training process. This Generalization Enhancement (GE) step restores and even boosts the model’s ability to generalize to unseen data, ensuring that balancing the modalities doesn’t come at the cost of performance.\nBuilding upon OGM, AGM [17] incorporates a Shapley value-based method to estimate the contribution of each modality and adjusts the gradient magnitudes accordingly. Prototypical Modality Rebalance (PMR) [18] modulates gradient strength based on category prototypes, thereby accelerating the learning process of weak modalities. InfoReg [19] identifies the different rates at which the model acquires information from each modality and constrains the gradient updates of dominant modalities at the batch level. By regulating the Fisher information of dominant modalities it helps to reduce imbalance during training.\nIn addition to gradient-based approaches, some methods adopt other optimization strategies to alleviate multimodal imbalance. ReconBoost [20] introduces the idea of boosting, a traditional technique in machine learning, as an optimization paradigm to enhance unimodal performance and, in turn, mitigate imbalance. Diagnosing \u0026amp; Re-learning (Relearning) [21] adopts a re-initialization policy to reduce over-dependence on dominant modalities while preventing weaker ones from overfitting to noise.\nFurther work related to multimodal imbalance The problem of modality imbalance in multimodal learning has been extensively studied from multiple perspectives, including Data input, Feed-forward, Learning objective, and Optimization. However, all of these approaches are developed within the paradigm of conventional multimodal learning frameworks. Recent research has begun to extend the study of modality imbalance to broader and more complex settings.\nBenchmark To systematically evaluate and compare different approaches for multimodal imbalance, BalanceBenchmark [5] has been introduced. This benchmark consolidates numerous existing methods and datasets, providing a unified coding framework that facilitates fair comparison and reproducibility. However, further large-scale experiments are needed, especially on Transformer-based encoder architectures, which are now prevalent. Additionally, more investigation into extreme imbalance scenarios, where one modality is significantly weaker or noisier than the others, should be conducted to test the boundaries of current methods.\nModality imbalance in Transformer RollingQ [22] identifies that when using Transformers for dynamic multimodal fusion, the model tends to favor certain modalities over others, assigning them higher attention scores and larger gradients during backpropagation. To address this imbalance, RollingQ proposes a method called rotating the query, which disrupts the existing modality preference. This technique rebalances attention allocation, gives underrepresented modalities more opportunity to be optimized, and ultimately revives cooperative learning dynamics between modalities.\nModality imbalance in MLLMs With the recent surge of interest in multimodal large models (MLLMs), a number of studies have begun to highlight the persistence of modality imbalance issues in this new context [23] [24] [25] [26] [27]. For instance, the work Chameleon [23] points out that competition naturally arises between modalities within large-scale multimodal model architectures. Some other studies [24] [25] [26] have begun to address this imbalance at the token level. A representative example is MokA [24], which observes a strong reliance on the text modality during parameter-efficient fine-tuning (PEFT) and proposes leveraging attention mechanisms to selectively extract highly correlated information from other modalities, thereby improving performance.\nThese works collectively demonstrate that modality imbalance remains a prevalent issue under current MLLMs framework. Since large language models (LLMs) often serve as the backbone of MLLMs, the text modality inherently becomes the dominant modality during training. In this context, future work should not only continue exploring modality-specific adjustments within the existing MLLMs framework, but also critically examine whether the framework itself is well-suited for fostering balanced multimodal intelligence and whether it can truly serve as the foundation for achieving artificial general intelligence (AGI).\nFuture research direction questions  While BalanceBenchmark [5] introduced the concept of \u0026ldquo;relative balance\u0026rdquo; (i.e., a trade-off point between performance and balance), it really gets us thinking: Does the blind pursuit of modality balance always pay off? Is this \u0026ldquo;relative balance\u0026rdquo; the same as \u0026ldquo;true balance\u0026rdquo;? And most importantly, how do we find that sweet spot in practice? Can we design a novel fusion architecture for multimodal large models that does not rely on a single dominant modality, especially text? Does a more equitable fusion mechanism exist that allows different modalities to interact more deeply and non-hierarchically at higher levels of the model, rather than simply \u0026ldquo;translating\u0026rdquo; visual or auditory information into textual concepts? How can we design new Parameter-Efficient Fine-Tuning (PEFT) methods that can balance updating and utilize information from all modalities during the fine-tuning process, rather than focusing only on text-related components? (MokA [24] has made some efforts!) How can we design a comprehensive evaluation benchmark for multimodal large models specifically to measure the contribution of different modalities and the model\u0026rsquo;s modality preferences? Can we develop explainability tools to visualize and analyze how information flows, fuses, and is ultimately utilized across modalities within a multimodal large model, thereby precisely identifying the bottlenecks of imbalance? In the progression towards AGI, how should a model learn to autonomously adjust its reliance on different modalities according to task demands to achieve true synergy and complementarity?  Join our discussion If you\u0026rsquo;re interested in multimodal imbalance and want to dive deeper into the discussion, we welcome you to join our community:\nGitHub: https://github.com/GeWu-Lab/awesome-balanced-multimodal-learning\nSlack Channel: https://join.slack.com/t/balancedmulti-mhs9373/shared_invite/zt-3aj46comq-CNWrkqqGI9Z2kNn64aU9FQ\nReferences [1] Adewumi, T., Alkhaled, L., Gurung, N., van Boven, G., \u0026amp; Pagliai, I. (2024). Fairness and bias in multimodal ai: A survey. arXiv preprint arXiv:2406.19097.\n[2] Zhang, Y., \u0026amp; Latham, P. (2024, July). Understanding Unimodal Bias in Multimodal Deep Linear Networks. In Proceedings of the 41st International Conference on Machine Learning (Vol. 235). PMLR.\n[3] Peng, X., Wei, Y., Deng, A., Wang, D., \u0026amp; Hu, D. (2022). Balanced multimodal learning via on-the-fly gradient modulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8238-8247).\n[4] Wei, Y., Hu, D., Du, H., \u0026amp; Wen, J. R. (2024). On-the-fly modulation for balanced multimodal learning. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n[5] Xu, S., Cui, M., Huang, C., Wang, H., \u0026amp; Hu, D. (2025). Balancebenchmark: A survey for multimodal imbalance learning. arXiv preprint arXiv:2502.10816.\n[6] Wei, Y., Feng, R., Wang, Z., \u0026amp; Hu, D. (2024). Enhancing multimodal cooperation via sample-level modality valuation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 27338-27347).\n[7] Zhang, X., Yoon, J., Bansal, M., \u0026amp; Yao, H. (2024). Multimodal representation learning by alternating unimodal adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 27456-27466).\n[8] Zhou, Y., Liang, X., Zheng, S., Xuan, H., \u0026amp; Kumada, T. (2023, June). Adaptive mask co-optimization for modal dependence in multimodal learning. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE.\n[9] Wu, N., Jastrzebski, S., Cho, K., \u0026amp; Geras, K. J. (2022, June). Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks. In International Conference on Machine Learning (pp. 24043-24055). PMLR.\n[10] Wei, Y., \u0026amp; Hu, D. (2024, July). MMPareto: boosting multimodal learning with innocent unimodal assistance. In Proceedings of the 41st International Conference on Machine Learning (pp. 52559-52572).\n[11] Xu, R., Feng, R., Zhang, S. X., \u0026amp; Hu, D. (2023, June). Mmcosine: Multi-modal cosine loss towards balanced audio-visual fine-grained learning. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE.\n[12] Liu, S., Li, L., Song, J., Yang, Y., \u0026amp; Zeng, X. (2023, February). Multimodal pre-training with self-distillation for product understanding in e-commerce. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (pp. 1039-1047).\n[13] Ma, H., Zhang, Q., Zhang, C., Wu, B., Fu, H., Zhou, J. T., \u0026amp; Hu, Q. (2023, July). Calibrating multimodal learning. In International Conference on Machine Learning (pp. 23429-23450). PMLR.\n[14] Yang, Y., Wan, F., Jiang, Q. Y., \u0026amp; Xu, Y. (2024). Facilitating multimodal classification via dynamically learning modality gap. Advances in Neural Information Processing Systems, 37, 62108-62122.\n[15] Du, C., Teng, J., Li, T., Liu, Y., Yuan, T., Wang, Y., \u0026hellip; \u0026amp; Zhao, H. (2023, July). On uni-modal feature learning in supervised multi-modal learning. In International Conference on Machine Learning (pp. 8632-8656). PMLR.\n[16] Wang, W., Tran, D., \u0026amp; Feiszli, M. (2020). What makes training multi-modal classification networks hard?. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12695-12705).\n[17] Li, H., Li, X., Hu, P., Lei, Y., Li, C., \u0026amp; Zhou, Y. (2023). Boosting multi-modal model performance with adaptive gradient modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 22214-22224).\n[18] Fan, Y., Xu, W., Wang, H., Wang, J., \u0026amp; Guo, S. (2023). Pmr: Prototypical modal rebalance for multimodal learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 20029-20038).\n[19] Huang, C., Wei, Y., Yang, Z., \u0026amp; Hu, D. (2025). Adaptive unimodal regulation for balanced multimodal information acquisition. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 25854-25863).\n[20] Hua, C., Xu, Q., Bao, S., Yang, Z., \u0026amp; Huang, Q. (2024, July). ReconBoost: Boosting Can Achieve Modality Reconcilement. In International Conference on Machine Learning (pp. 19573-19597). PMLR.\n[21] Wei, Y., Li, S., Feng, R., \u0026amp; Hu, D. (2024, September). Diagnosing and re-learning for balanced multimodal learning. In European Conference on Computer Vision (pp. 71-86). Cham: Springer Nature Switzerland.\n[22] Ni, H., Wei, Y., Liu, H., Chen, G., Peng, C., Lin, H., \u0026amp; Hu, D. RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer. In Forty-second International Conference on Machine Learning.\n[23] Team, C. (2024). Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818.\n[24] Wei, Y., Miao, Y., Zhou, D., \u0026amp; Hu, D. (2025). MokA: Multimodal Low-Rank Adaptation for MLLMs. In Advances in Neural Information Processing Systems (NeurIPS).\n[25] Jiang, T., Song, M., Zhang, Z., Huang, H., Deng, W., Sun, F., \u0026hellip; \u0026amp; Zhuang, F. (2024). E5-V: Universal Embeddings with Multimodal Large Language Models. CoRR.\n[26] Jung, C., Jang, Y., Choi, J., \u0026amp; Chung, J. S. (2025). Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models. arXiv preprint arXiv:2505.20873.\n[27] Park, S., Panigrahi, A., Cheng, Y., Yu, D., Goyal, A., \u0026amp; Arora, S. Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?. In Forty-second International Conference on Machine Learning.\n","date":1760054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1760054400,"objectID":"e176c0b1f2b56206e659ec9567cfbcea","permalink":"/blog/balanced-multimodal-learning/","publishdate":"2025-10-10T00:00:00Z","relpermalink":"/blog/balanced-multimodal-learning/","section":"blog","summary":"Understanding modality imbalance in multimodal learning and how to address it","tags":["multimodal learning","deep learning"],"title":"Balanced Multimodal Learning","type":"blog"},{"authors":["Rui Qian","Di Hu","Heinrich Dinkel","Mengyue Wu","Ning Xu","Weiyao Lin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"852b491b0dcadb44b8f099f931db74c4","permalink":"/publication/a-two-stage-framework-for-multiple-sound-source-localization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/a-two-stage-framework-for-multiple-sound-source-localization/","section":"publication","summary":"","tags":null,"title":"A Two-Stage Framework for Multiple Sound-Source Localization","type":"publication"},{"authors":["Chengxiang Huang","Yake Wei","Zequn Yang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a8b66e62cb78802e805a87de546c7a2c","permalink":"/publication/adaptive-unimodal-regulation-for-balanced-multimodal-information-acquisition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/adaptive-unimodal-regulation-for-balanced-multimodal-information-acquisition/","section":"publication","summary":"","tags":null,"title":"Adaptive Unimodal Regulation for Balanced Multimodal Information Acquisition","type":"publication"},{"authors":["Di Hu*","Lichao Mou*","Qingzhong Wang*","Junyu Gao","Yuansheng Hua","Dejing Dou","Xiao Xiang Zhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b21459d2cd2aa98d5a771a396df3c29e","permalink":"/publication/ambient-sound-helps_-audiovisual-crowd-counting-in-extreme-conditions/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ambient-sound-helps_-audiovisual-crowd-counting-in-extreme-conditions/","section":"publication","summary":"","tags":null,"title":"Ambient Sound Helps: Audiovisual Crowd Counting in Extreme Conditions","type":"publication"},{"authors":["Ruoxuan Feng","Jiangyu Hu","Wenke Xia","Tianci Gao","Ao Shen","Yuhao Sun","Bin Fang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"54bf11452eb8a847020959425c098dd9","permalink":"/publication/anytouch-learning-unified-static-dynamic-representation-across-multiple-visuo-tactile-sensors/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/anytouch-learning-unified-static-dynamic-representation-across-multiple-visuo-tactile-sensors/","section":"publication","summary":"","tags":null,"title":"AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors","type":"publication"},{"authors":["Wenke Xia*","Xu Zhao*","Xincheng Pang","Changqing Zhang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7a5ff9681de843469038165a230c4f87","permalink":"/publication/balanced-audiovisual-dataset-for-imbalance-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/balanced-audiovisual-dataset-for-imbalance-analysis/","section":"publication","summary":"","tags":null,"title":"Balanced Audiovisual Dataset for Imbalance Analysis","type":"publication"},{"authors":["Xiaokang Peng*","Yake Wei*","Andong Deng","Dong Wang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1cdda2159c4adeb4f31cb4e7f1a5ab8a","permalink":"/publication/balanced-multimodal-learning-via-on-the-fly-gradient-modulation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/balanced-multimodal-learning-via-on-the-fly-gradient-modulation/","section":"publication","summary":"","tags":null,"title":"Balanced Multimodal Learning via On-the-fly Gradient Modulation","type":"publication"},{"authors":["Guangyao Li","Henghui Du","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f94c6c686cdea2a7e59df024a1411beb","permalink":"/publication/boosting-audio-visual-question-answering-via-key-semantic-aware-cues/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/boosting-audio-visual-question-answering-via-key-semantic-aware-cues/","section":"publication","summary":"","tags":null,"title":"Boosting Audio Visual Question Answering via Key Semantic-Aware Cues","type":"publication"},{"authors":["Yaoting Wang*","Peiwen Sun*","Yuanchao Li","Honggang Zhang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a81f1d27e2c20becc702c936eff80aa0","permalink":"/publication/can-textual-semantics-mitigate-sounding-object-segmentationpreference_/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/can-textual-semantics-mitigate-sounding-object-segmentationpreference_/","section":"publication","summary":"","tags":null,"title":"Can Textual Semantics Mitigate Sounding Object SegmentationPreference?","type":"publication"},{"authors":["Di Hu","Yake Wei","Rui Qian","Weiyao Lin","Ruihua Song","Ji-Rong Wen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"91e67073102678aec9799732ceef49f3","permalink":"/publication/class-aware-sounding-objects-localization-via-audiovisual-correspondence/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/class-aware-sounding-objects-localization-via-audiovisual-correspondence/","section":"publication","summary":"","tags":null,"title":"Class-aware Sounding Objects Localization via Audiovisual Correspondence","type":"publication"},{"authors":["Yapeng Tian*","Di Hu*","Chenliang Xu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c0d82a52007e4e9ab50a2cfafdc4ac17","permalink":"/publication/co-learn-sounding-object-visual-grounding-and-visually-indicated-sound-separation-in-a-cycle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/co-learn-sounding-object-visual-grounding-and-visually-indicated-sound-separation-in-a-cycle/","section":"publication","summary":"","tags":null,"title":"Co-Learn Sounding Object Visual Grounding and Visually Indicated Sound Separation in A Cycle","type":"publication"},{"authors":["Di Hu","Xuhong Li","Lichao Mou","Pu Jin","Dong Chen","Liping Jing","Xiaoxiang Zhu","Dejing Dou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c7688dd14aa743d0b927f94d97854f27","permalink":"/publication/cross-task-transfer-for-geotagged-audiovisual-aerial-scene-recognition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/cross-task-transfer-for-geotagged-audiovisual-aerial-scene-recognition/","section":"publication","summary":"","tags":null,"title":"Cross-Task Transfer for Geotagged Audiovisual Aerial Scene Recognition","type":"publication"},{"authors":["Di Hu","Zheng Wang","Haoyi Xiong","Dong Wang","Feiping Nie","Dejing Dou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ac02b15b850ff085e6c9ad497f3a130c","permalink":"/publication/curriculum-audiovisual-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/curriculum-audiovisual-learning/","section":"publication","summary":"","tags":null,"title":"Curriculum Audiovisual Learning","type":"publication"},{"authors":["Yapeng Tian","Di Hu","Chenliang Xu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cd0308a1bfb55705c394057955f2375d","permalink":"/publication/cyclic-co-learning-of-sounding-object-visual-grounding-and-sound-separation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/cyclic-co-learning-of-sounding-object-visual-grounding-and-sound-separation/","section":"publication","summary":"","tags":null,"title":"Cyclic Co-Learning of Sounding Object Visual Grounding and Sound Separation","type":"publication"},{"authors":["Di Hu","Feiping Nie","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"202776673a51788c119f1451c9e313c2","permalink":"/publication/deep-binary-reconstruction-for-cross-modal-hashing-journal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/deep-binary-reconstruction-for-cross-modal-hashing-journal/","section":"publication","summary":"","tags":null,"title":"Deep Binary Reconstruction for Cross-modal Hashing","type":"publication"},{"authors":["Di Hu","Feiping Nie","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"00f72a8fe1deeb265958a59b94c2cd33","permalink":"/publication/deep-binary-reconstruction-for-cross-modal-hashing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/deep-binary-reconstruction-for-cross-modal-hashing/","section":"publication","summary":"","tags":null,"title":"Deep Binary Reconstruction for Cross-modal Hashing","type":"publication"},{"authors":["Di Hu","Feiping Nie","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f6c0a9a658cdceee78bd291860181d99","permalink":"/publication/deep-linear-discriminant-analysis-hashing-supplemental-material/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/deep-linear-discriminant-analysis-hashing-supplemental-material/","section":"publication","summary":"","tags":null,"title":"Deep Linear Discriminant Analysis Hashing","type":"publication"},{"authors":["Di Hu","Feiping Nie","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d1466a6c42ba930502049d24243f8b62","permalink":"/publication/deep-multimodal-clustering-for-unsupervised-audiovisual-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/deep-multimodal-clustering-for-unsupervised-audiovisual-learning/","section":"publication","summary":"","tags":null,"title":"Deep Multimodal Clustering for Unsupervised Audiovisual Learning Representation","type":"publication"},{"authors":["Di Hu - Chengze Wang - Feiping Nie - Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e4cd76d6b972d54b50c190779f639a5","permalink":"/publication/dense-multimodal-fusion-for-hierarchically-joint-representation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/dense-multimodal-fusion-for-hierarchically-joint-representation/","section":"publication","summary":"","tags":null,"title":"Dense Multimodal Fusion for Hierarchically Joint Representation","type":"publication"},{"authors":["Xincheng Pang","Wenke Xia","Zhigang Wang","Bin Zhao","Di Hu","Dong Wang","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4cb0ba95f2909493767a29fa7cfc4f6c","permalink":"/publication/depth-helps_-improving-pre-trained-rgb-based-policy-with-depth-information-injection/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/depth-helps_-improving-pre-trained-rgb-based-policy-with-depth-information-injection/","section":"publication","summary":"","tags":null,"title":"Depth Helps: Improving Pre-trained RGB-based Policy with Depth Information Injection","type":"publication"},{"authors":["Yake Wei","Siwei Li","Ruoxuan Feng","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"591c348a8e03f441318436eb005ae2cc","permalink":"/publication/diagnosing-and-re-learning-for-balanced-multimodal-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/diagnosing-and-re-learning-for-balanced-multimodal-learning/","section":"publication","summary":"","tags":null,"title":"Diagnosing and Re-learning for Balanced Multimodal Learning","type":"publication"},{"authors":["Di Hu","Feiping Nie","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4b1e10b4327cca00dfd58162571a2f8c","permalink":"/publication/discrete-spectral-hashing-for-efficient-similarity-retrieval/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/discrete-spectral-hashing-for-efficient-similarity-retrieval/","section":"publication","summary":"","tags":null,"title":"Discrete Spectral Hashing for Efficient Similarity Retrieval","type":"publication"},{"authors":["Di Hu","Rui Qian","Minyue Jiang","Xiao Tan","Shilei Wen","Errui Ding","Weiyao Lin","Dejing Dou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d6953eeac03ee85322e85eece2eeeb84","permalink":"/publication/discriminative-sounding-objects-localization-via-self-supervised-audiovisual-matching/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/discriminative-sounding-objects-localization-via-self-supervised-audiovisual-matching/","section":"publication","summary":"","tags":null,"title":"Discriminative Sounding Objects Localization via Self-supervised Audiovisual Matching","type":"publication"},{"authors":["Di Hu*","Lichao Mou*","Qingzhong Wang*","Junyu Gao","Yuansheng Hua","Dejing Dou","Xiaoxiang Zhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3016d01c7b86e792f8778f7aba6fc44d","permalink":"/publication/does-ambient-sound-help_-audiovisual-crowd-counting/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/does-ambient-sound-help_-audiovisual-crowd-counting/","section":"publication","summary":"","tags":null,"title":"Does Ambient Sound Help? - Audiovisual Crowd Counting","type":"publication"},{"authors":["Zequn Yang","Hongfa Wang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"df33764efe082d9b338ca9a7bef665e8","permalink":"/publication/efficient-quantification-of-multimodal-interaction-at-sample-level/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/efficient-quantification-of-multimodal-interaction-at-sample-level/","section":"publication","summary":"","tags":null,"title":"Efficient Quantification of Multimodal Interaction at Sample Level","type":"publication"},{"authors":["Yake Wei","Ruoxuan Feng","Zihe Wang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0278c6a7c52909fa5c55eaf522569e7f","permalink":"/publication/enhancing-multi-modal-cooperation-via-fine-grained-modality-valuation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/enhancing-multi-modal-cooperation-via-fine-grained-modality-valuation/","section":"publication","summary":"","tags":null,"title":"Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation","type":"publication"},{"authors":["Xinchi Zhou","Dongzhan Zhou","Di Hu","Hang Zhou","Wanli Ouyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"28bd51450c42258842f48363910f83c8","permalink":"/publication/exploiting-visual-context-semantics-for-sound-source-localization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/exploiting-visual-context-semantics-for-sound-source-localization/","section":"publication","summary":"","tags":null,"title":"Exploiting Visual Context Semantics for Sound Source Localization","type":"publication"},{"authors":["Sijia Yang","Haoyi Xiong","Di Hu","Kaibo Xu","Licheng Wang","Peizhen Zhu","Zeyi Sun"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ed52bf34eef1f16fc89a0fc5c32fa152","permalink":"/publication/generalising-combinatorial-discriminant-analysis-through-conditioning-truncated-rayleigh-flow/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/generalising-combinatorial-discriminant-analysis-through-conditioning-truncated-rayleigh-flow/","section":"publication","summary":"","tags":null,"title":"Generalising Combinatorial Discriminant Analysis through Conditioning Truncated Rayleigh Flow","type":"publication"},{"authors":["Zequn Yang","Han Zhang","Yake Wei","Zheng Wang","Feiping Nie","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"76c72a76e4cf8516d166a780e270c79b","permalink":"/publication/geometric-inspired-graph-based-incomplete-multi-view-clustering/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/geometric-inspired-graph-based-incomplete-multi-view-clustering/","section":"publication","summary":"","tags":null,"title":"Geometric-Inspired Graph-based Incomplete Multi-view Clustering","type":"publication"},{"authors":["Di Hu","Zheng Wang","Haoyi Xiong","Dong Wang","Feiping Nie","Dejing Dou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8fe03bbbdab04c3ee4ecc7e01ecd723c","permalink":"/publication/heterogeneous-scene-analysis-via-self-supervised-audiovisual-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/heterogeneous-scene-analysis-via-self-supervised-audiovisual-learning/","section":"publication","summary":"","tags":null,"title":"Heterogeneous Scene Analysis via Self-supervised Audiovisual Learning","type":"publication"},{"authors":["Xuelong Li","Di Hu","Xiaoqiang Lu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1850ab6a7473c571586aed28d796ac66","permalink":"/publication/image2song-song-retrieval-via-bridging-image-content-and-lyric-words/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/image2song-song-retrieval-via-bridging-image-content-and-lyric-words/","section":"publication","summary":"","tags":null,"title":"Image2song: Song Retrieval via Bridging Image Content and Lyric Words","type":"publication"},{"authors":["Wenke Xia","Dong Wang","Xincheng Pang","Zhigang Wang","Bin Zhao","Di Hu","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"82a334df3b6181644b600e4679ce595c","permalink":"/publication/kinematic-aware-prompting-for-generalizable-articulated-object-manipulation-with-llms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/kinematic-aware-prompting-for-generalizable-articulated-object-manipulation-with-llms/","section":"publication","summary":"","tags":null,"title":"Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs","type":"publication"},{"authors":["Jingxian Lu","Wenke Xia","Dong Wang","Zhigang Wang","Bin Zhao","Di Hu","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aef6217c80235f04d442653564108582","permalink":"/publication/koi_-accelerating-online-imitation-learning-via-hybrid-key-state-guidance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/koi_-accelerating-online-imitation-learning-via-hybrid-key-state-guidance/","section":"publication","summary":"","tags":null,"title":"KOI: Accelerating Online Imitation Learning via Hybrid Key-state Guidance","type":"publication"},{"authors":["Xuelong Li","Di Hu","Feiping Nie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"24881bb5f959ea9f061fb67469d72eb9","permalink":"/publication/large-graph-hashing-with-spectral-rotation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/large-graph-hashing-with-spectral-rotation/","section":"publication","summary":"","tags":null,"title":"Large Graph Hashing with Spectral Rotation","type":"publication"},{"authors":["Yake Wei","Di Hu","Yapeng Tian","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a74e00cc3fbe94bd488a1b2e2c9dec3b","permalink":"/publication/learning-in-audio-visual-context_-a-review-analysis-and-new-perspective/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/learning-in-audio-visual-context_-a-review-analysis-and-new-perspective/","section":"publication","summary":"","tags":null,"title":"Learning in Audio-visual Context: A Review, Analysis, and New Perspective","type":"publication"},{"authors":["Guangyao Li*","Yake Wei*","Yapeng Tian*","Chenliang Xu","Ji-Rong Wen","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"125a97cdaa82fb5a0ec455cfd53c1b46","permalink":"/publication/learning-to-answer-questions-in-dynamic-audio-visual-scenarios/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/learning-to-answer-questions-in-dynamic-audio-visual-scenarios/","section":"publication","summary":"","tags":null,"title":"Learning to Answer Questions in Dynamic Audio-Visual Scenarios","type":"publication"},{"authors":["Di Hu","Dong Wang","Xuelong Li","Feiping Nie","Qi Wang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c716bb52e5e46a2dbaebc46fda1517d6","permalink":"/publication/listen-to-the-image/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/listen-to-the-image/","section":"publication","summary":"","tags":null,"title":"Listen to the Image","type":"publication"},{"authors":["Ruize Xu","Ruoxuan Feng","Shi-xiong Zhang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d7c43566e1b5933f9b5354402f3fc016","permalink":"/publication/mmcosine_-multi-modal-cosine-loss-towards-balanced-audio-visual-fine-grained-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/mmcosine_-multi-modal-cosine-loss-towards-balanced-audio-visual-fine-grained-learning/","section":"publication","summary":"","tags":null,"title":"MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual Fine-Grained Learning","type":"publication"},{"authors":["Yake Wei","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"57b9c8b24f0e51d7c6ebbcdab9df8b5e","permalink":"/publication/mmpareto_-innocent-uni-modal-assistance-for-enhanced-multi-modal-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/mmpareto_-innocent-uni-modal-assistance-for-enhanced-multi-modal-learning/","section":"publication","summary":"","tags":null,"title":"MMPareto: Innocent Uni-modal Assistance for Enhanced Multi-modal Learning","type":"publication"},{"authors":["Guangyao Li","Yixin Xu","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"38daed7d60d2831123ddca90ac47d9b7","permalink":"/publication/multi-scale-attention-for-audio-question-answering/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/multi-scale-attention-for-audio-question-answering/","section":"publication","summary":"","tags":null,"title":"Multi-Scale Attention for Audio Question Answering","type":"publication"},{"authors":["Rui Qian","Di Hu","Heinrich Dinkel","Mengyue Wu","Ning Xu","Weiyao Lin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"88c9d48496c44a5980763aa946676e9e","permalink":"/publication/multiple-sound-sources-localization-from-coarse-to-fine/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/multiple-sound-sources-localization-from-coarse-to-fine/","section":"publication","summary":"","tags":null,"title":"Multiple Sound Sources Localization from Coarse to Fine","type":"publication"},{"authors":["Ziyun Li","Xinshao Wang","Haojin Yang","Di Hu","Neil M Robertson","David A Clifton","Christoph Meinel","Haojin Yang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a48ea4ca10463e6ef980903ef312977d","permalink":"/publication/not-all-knowledge-is-created-equal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/not-all-knowledge-is-created-equal/","section":"publication","summary":"","tags":null,"title":"Not All Knowledge Is Created Equal","type":"publication"},{"authors":["Yake Wei","Di Hu","Henghui Du","Ji-Rong Wen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b7408ca6935cac938e3215e74312bfa7","permalink":"/publication/on-the-fly-modulation-for-balanced-multimodal-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/on-the-fly-modulation-for-balanced-multimodal-learning/","section":"publication","summary":"","tags":null,"title":"On-the-fly Modulation for Balanced Multimodal Learning","type":"publication"},{"authors":["Ruotian Peng*","Haiying He*","Yake Wei","Yandong Wen","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2f559356ab10ceeadfe0cec37084844e","permalink":"/publication/patch-matters-training-free-fine-grained-image-caption-enhancement-via-local-perception/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/patch-matters-training-free-fine-grained-image-caption-enhancement-via-local-perception/","section":"publication","summary":"","tags":null,"title":"Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception","type":"publication"},{"authors":["Wenke Xia","Ruoxuan Feng","Dong Wang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d5e4ff7831012ff163fc56988df03fae","permalink":"/publication/phoenix-a-motion-based-self-reflection-framework-for-fine-grained-robotic-action-correction-/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/phoenix-a-motion-based-self-reflection-framework-for-fine-grained-robotic-action-correction-/","section":"publication","summary":"","tags":null,"title":"Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction","type":"publication"},{"authors":["Guangyao Li","Wenxuan Hou","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"545100c95da731d9faeb7037b5801449","permalink":"/publication/progressive-spatio-temporal-perception-for-audio-visual-question-answering/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/progressive-spatio-temporal-perception-for-audio-visual-question-answering/","section":"publication","summary":"","tags":null,"title":"Progressive Spatio-temporal Perception for Audio-Visual Question Answering","type":"publication"},{"authors":["Yaoting Wang*","Weisong Liu*","Guangyao Li","Jian Ding","Di Hu","Xi Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d414aab41857970b60155d360ceac88","permalink":"/publication/prompting-segmentation-with-sound-is-generalizable-audio-visual-source-localizer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/prompting-segmentation-with-sound-is-generalizable-audio-visual-source-localizer/","section":"publication","summary":"","tags":null,"title":"Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer","type":"publication"},{"authors":["Zequn Yang","Yake Wei","Ce Liang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d069d78586930bf2dd726ae7c0b00c9b","permalink":"/publication/quantifying-and-enhancing-multi-modal-robustness-with-modality-preference/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/quantifying-and-enhancing-multi-modal-robustness-with-modality-preference/","section":"publication","summary":"","tags":null,"title":"Quantifying and Enhancing Multi-modal Robustness with Modality Preference","type":"publication"},{"authors":["Yaoting Wang*","Peiwen Sun*","Dongzhan Zhou","Guangyao Li","Honggang Zhang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"af23c065a05a920d8deda6902d475730","permalink":"/publication/ref-avs_-refer-and-segment-objects-in-audio-visual-scenes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/ref-avs_-refer-and-segment-objects-in-audio-visual-scenes/","section":"publication","summary":"","tags":null,"title":"Ref-AVS: Refer and Segment Objects in Audio-Visual Scenes","type":"publication"},{"authors":["Ruoxuan Feng","Wenke Xia","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"450f11c7cb976aa1013ed40cd3963388","permalink":"/publication/revisiting-pre-training-in-audio-visual-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/revisiting-pre-training-in-audio-visual-learning/","section":"publication","summary":"","tags":null,"title":"Revisiting Pre-training in Audio-Visual Learning","type":"publication"},{"authors":["Haotian Ni","Yake Wei","Hang Liu","Gong Chen","Chong Peng","Hao Lin","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"86ffc360bd790bd3ef22266138d04d2f","permalink":"/publication/reviving-the-cooperation-dynamics-in-multimodal-transformer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/reviving-the-cooperation-dynamics-in-multimodal-transformer/","section":"publication","summary":"","tags":null,"title":"Reviving the Cooperation Dynamics in Multimodal Transformer","type":"publication"},{"authors":["Wenke Xia","Xingjian Li","Andong Deng","Haoyi Xiong","Dejing Dou","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d68814ab18c4fd432535b2592c31988","permalink":"/publication/robust-cross-modal-knowledge-distillation-for-unconstrained-videos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/robust-cross-modal-knowledge-distillation-for-unconstrained-videos/","section":"publication","summary":"","tags":null,"title":"Robust Cross-modal Knowledge Distillation for Unconstrained Videos","type":"publication"},{"authors":["Xinchi Zhou","Dongzhan Zhou","Wanli Ouyang","Hang Zhou","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f2cabda8aceb0e7aac3a738a7cd5821e","permalink":"/publication/seco_-separating-unknown-musical-visual-sounds-with-consistency-guidance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/seco_-separating-unknown-musical-visual-sounds-with-consistency-guidance/","section":"publication","summary":"","tags":null,"title":"SeCo: Separating Unknown Musical Visual Sounds with Consistency Guidance","type":"publication"},{"authors":["Konrad Heidler","Lichao Mou","Di Hu","Pu Jin","Guangyao Li","Chuang Gan","Ji-Rong Wen","Xiao Xiang Zhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"36c9fd21445495f69bad705471393094","permalink":"/publication/self-supervised-audiovisual-representation-learning-for-remote-sensing-data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/self-supervised-audiovisual-representation-learning-for-remote-sensing-data/","section":"publication","summary":"","tags":null,"title":"Self-supervised Audiovisual Representation Learning for Remote Sensing Data","type":"publication"},{"authors":["Di Hu","Zheng Wang","Feiping Nie","Rong Wang","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ac1ac86aa9c1772d446b7594a05d9100","permalink":"/publication/self-supervised-learning-for-heterogeneous-audiovisual-scene-analysis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/self-supervised-learning-for-heterogeneous-audiovisual-scene-analysis/","section":"publication","summary":"","tags":null,"title":"Self-supervised Learning for Heterogeneous Audiovisual Scene Analysis","type":"publication"},{"authors":["Dongzhan Zhou","Xinchi Zhou","Di Hu","Hang Zhou","Lei Bai","Ziwei Liu","Wanli Ouyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3f2c9d5779b3cec3c9b69a845335b218","permalink":"/publication/sepfusion_-finding-optimal-fusion-structures-for-visual-sound-separation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/sepfusion_-finding-optimal-fusion-structures-for-visual-sound-separation/","section":"publication","summary":"","tags":null,"title":"SepFusion: Finding Optimal Fusion Structures for Visual Sound Separation","type":"publication"},{"authors":["Tao Wu","Xuewei Li","Zhongang Qi","Di Hu","Xintao Wang","Ying Shan","Xi Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"3ad4a97eaccd1f83d6453be7a0f4a174","permalink":"/publication/spherediffusion_-spherical-geometry-aware-distortion-resilient-diffusion-model/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/spherediffusion_-spherical-geometry-aware-distortion-resilient-diffusion-model/","section":"publication","summary":"","tags":null,"title":"SphereDiffusion: Spherical Geometry-aware Distortion Resilient Diffusion Model","type":"publication"},{"authors":["Juncheng Ma","Peiwen Sun","Yaoting Wang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"db4915fbc65e916e7b2683ab5803c982","permalink":"/publication/stepping-stones_-a-progressive-training-strategy-for-audio-visual-semantic-segmentation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/stepping-stones_-a-progressive-training-strategy-for-audio-visual-semantic-segmentation/","section":"publication","summary":"","tags":null,"title":"Stepping Stones: A Progressive Training Strategy for Audio-Visual Semantic Segmentation","type":"publication"},{"authors":["ZiYun Li","Jona Otholt","Ben Dai","Di Hu","Christoph Meinel","Haojin Yang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"50b53591fe6d761222acbe7d191d3e47","permalink":"/publication/supervised-knowledge-may-hurt-novel-class-discovery-performance/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/supervised-knowledge-may-hurt-novel-class-discovery-performance/","section":"publication","summary":"","tags":null,"title":"Supervised Knowledge May Hurt Novel Class Discovery Performance","type":"publication"},{"authors":["Di Hu","Xuelong Li","Xiaoqiang Lu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d6704b0eb55495bb979be6fcbb8243ae","permalink":"/publication/temporal-multimodal-learning-in-audiovisual-speech-recognition/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/temporal-multimodal-learning-in-audiovisual-speech-recognition/","section":"publication","summary":"","tags":null,"title":"Temporal Multimodal Learning in Audiovisual Speech Recognition","type":"publication"},{"authors":["Dong Wang","Di Hu","Xingjian Li","Dejing Dou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"807bb234ac2724175550dbdf52f64d08","permalink":"/publication/temporal-relational-modeling-with-self-supervision-for-action-segmentation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/temporal-relational-modeling-with-self-supervision-for-action-segmentation/","section":"publication","summary":"","tags":null,"title":"Temporal Relational Modeling with Self-Supervision for Action Segmentation","type":"publication"},{"authors":["Hongpeng Lin*","Ludan Ruan*","Wenke Xia*","Peiyu Liu","Jingyuan Wen","Yixin Xu","Di Hu","Ruihua Song","Wayne Xin Zhao","Qin Jin","Zhiwu Lu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cecd5dced0b37d8fe61dc6ba9f2b1491","permalink":"/publication/tiktalk_-a-video-based-dialogue-dataset-for-multi-modal-chitchat-in-real-world/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/tiktalk_-a-video-based-dialogue-dataset-for-multi-modal-chitchat-in-real-world/","section":"publication","summary":"","tags":null,"title":"TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World","type":"publication"},{"authors":["Xingjian Li","Di Hu","Xuhong Li","Haoyi Xiong","Zhi Ye","Zhipeng Wang","Chengzhong Xu","Dejing Dou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5b40a464bbfccb601c6d4c37e85cf81e","permalink":"/publication/towards-accurate-knowledge-transfer-via-target-awareness-representation-disentanglement/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/towards-accurate-knowledge-transfer-via-target-awareness-representation-disentanglement/","section":"publication","summary":"","tags":null,"title":"Towards Accurate Knowledge Transfer via Target-awareness Representation Disentanglement","type":"publication"},{"authors":["Andong Deng","Xingjian Li","Di Hu","Tianyang Wang","Haoyi Xiong","Chengzhong Xu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"113edd12d767a54c1fdd10685167cd5c","permalink":"/publication/towards-inadequately-pre-trained-models-in-transfer-learning/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/towards-inadequately-pre-trained-models-in-transfer-learning/","section":"publication","summary":"","tags":null,"title":"Towards Inadequately Pre-trained Models in Transfer Learning","type":"publication"},{"authors":["Wenxuan Hou*","Guangyao Li*","Yapeng Tian","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"56c2e256bf8d4a20cdffe034f430aaef","permalink":"/publication/towards-long-form-audio-visual-video-understanding/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/towards-long-form-audio-visual-video-understanding/","section":"publication","summary":"","tags":null,"title":"Towards Long Form Audio-visual Video Understanding","type":"publication"},{"authors":["Zechen Bai","Zhigang Wang","Jian Wang","Di Hu","Errui Ding"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9905f139a565b4f5eabfc5902965f851","permalink":"/publication/unsupervised-multi-source-domain-adaptation-for-person-re-identification/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/unsupervised-multi-source-domain-adaptation-for-person-re-identification/","section":"publication","summary":"","tags":null,"title":"Unsupervised Multi-Source Domain Adaptation for Person Re-Identification","type":"publication"},{"authors":["Peiwen Sun","Honggang Zhang","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6ff959ec7e9a3da6203370e48a939fd1","permalink":"/publication/unveiling-and-mitigating-bias-in-audio-visual-segmentation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/unveiling-and-mitigating-bias-in-audio-visual-segmentation/","section":"publication","summary":"","tags":null,"title":"Unveiling and Mitigating Bias in Audio Visual Segmentation","type":"publication"},{"authors":["Xian Liu","Rui Qian","Hang Zhou","Di Hu","Weiyao Lin","Ziwei Liu","Bolei Zhou","Xiaowei Zhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ca462fd19e2017e2ecb2b26a145ef250","permalink":"/publication/visual-sound-localization-in-the-wild-by-cross-modal-interference-erasing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/visual-sound-localization-in-the-wild-by-cross-modal-interference-erasing/","section":"publication","summary":"","tags":null,"title":"Visual Sound Localization in-the-Wild by Cross-Modal Interference Erasing","type":"publication"},{"authors":["Henghui Du","Guangyao Li","Chang Zhou","Chunjie Zhang","Alan Zhao","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a9d4ac3b9400c1397727fa59f0a980ce","permalink":"/publication/crab-a-unified-audio-visual-scene-understanding-model-with-explicit-cooperation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/crab-a-unified-audio-visual-scene-understanding-model-with-explicit-cooperation/","section":"publication","summary":"","tags":null,"title":"🔥Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation","type":"publication"},{"authors":["Yake Wei","Yu Miao","Dongzhan Zhou","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"326ca72d7127ba5173fe8ab1cf20eabc","permalink":"/publication/moka-multimodal-low-rank-adaptation-for-mllms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/moka-multimodal-low-rank-adaptation-for-mllms/","section":"publication","summary":"","tags":null,"title":"🔥MokA: Multimodal Low-Rank Adaptation for MLLMs","type":"publication"},{"authors":["Ruoxuan Feng","Di Hu","Wenke Ma","Xuelong Li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"456f98e9320bfba3ae79c7ab6d82a9cf","permalink":"/publication/play-to-the-score_-stage-guided-dynamic-multi-sensory-fusion-for-robotic-manipulation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/play-to-the-score_-stage-guided-dynamic-multi-sensory-fusion-for-robotic-manipulation/","section":"publication","summary":"","tags":null,"title":"🔥Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation","type":"publication"},{"authors":["Wenke Xia","Yichu Yang","Hongtao Wu","Xiao Ma","Tao Kong","Di Hu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"60ebb4ceeb28427cf9442635f54ed10e","permalink":"/publication/robotic-policy-learning-via-human-assisted-action-preference-optimization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/robotic-policy-learning-via-human-assisted-action-preference-optimization/","section":"publication","summary":"","tags":null,"title":"🔥Robotic Policy Learning via Human-assisted Action Preference Optimization","type":"publication"}]